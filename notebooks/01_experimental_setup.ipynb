{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c10bfc8",
   "metadata": {},
   "source": [
    "# Kísérleti beállítások és módszertan\n",
    "\n",
    "Ebben a beadandóban a **SARSA** és **Q-learning** algoritmusokat hasonlítom össze\n",
    "két klasszikus megerősítéses tanulási környezeten:\n",
    "\n",
    "- **FrozenLake-v1** – diszkrét állapottér\n",
    "- **CartPole-v1** – folytonos állapottér, tabulárisan diszkretizálva\n",
    "\n",
    "A cél annak vizsgálata, hogy az **on-policy** és **off-policy** tanulás\n",
    "milyen különbségeket eredményez különböző környezeti feltételek mellett.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e75c3",
   "metadata": {},
   "source": [
    "## Vizsgált algoritmusok\n",
    "\n",
    "### SARSA (on-policy)\n",
    "A SARSA algoritmus a frissítés során a ténylegesen végrehajtott következő akciót\n",
    "használja, így a tanult értékfüggvény közvetlenül a követett policy-hez kötődik.\n",
    "\n",
    "\\[\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma Q(s', a') - Q(s,a))\n",
    "\\]\n",
    "\n",
    "### Q-learning (off-policy)\n",
    "A Q-learning algoritmus a következő állapotban elérhető maximális Q-értéket\n",
    "használja, függetlenül a ténylegesen végrehajtott akciótól.\n",
    "\n",
    "\\[\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma \\max_a Q(s', a) - Q(s,a))\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901f458",
   "metadata": {},
   "source": [
    "## Reprodukálhatóság\n",
    "\n",
    "Minden kísérlet több véletlen seed-del került lefuttatásra.\n",
    "Az eredmények seed-ek felett kerülnek átlagolásra, a szórás feltüntetésével.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl)",
   "language": "python",
   "name": "rl-venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
